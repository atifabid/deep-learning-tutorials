{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 3 ANN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i5gLfdFcQiJU"
   },
   "source": [
    "Step 1: Importing Necessary Libraries\n",
    "\n",
    "First, import the required libraries for building and training the neural network. These include NumPy for numerical operations, Matplotlib for data visualization, and modules from TensorFlow/Keras to handle the dataset, model creation, and layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BMatvCkcPLz9"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wd54cT9XQxec"
   },
   "source": [
    "Step 2: Loading and Preprocessing the MNIST Dataset\n",
    "\n",
    "This step involves loading the MNIST dataset and preparing it for the neural network. It consists of three parts: loading the data, normalizing the pixel values from a range of 0-255 to 0-1, and one-hot encoding the integer labels into a binary matrix format. For example, the label '3' is converted to\n",
    "\n",
    "[0, 0, 0, 1, 0, 0, 0, 0, 0, 0]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "s3vvnpcMQnxm",
    "outputId": "07a7388f-5fc7-4f7a-c1e5-1d04e292e8e7"
   },
   "outputs": [],
   "source": [
    "# Load the MNIST dataset, splitting it into training and test sets\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Normalize pixel values to the range [0, 1] for stable and efficient training\n",
    "x_train = x_train.astype('float32') / 255.0\n",
    "x_test = x_test.astype('float32') / 255.0\n",
    "\n",
    "# One-hot encode the labels (0-9) for categorical classification\n",
    "y_train = to_categorical(y_train, 10)\n",
    "y_test = to_categorical(y_test, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YKTQRdvOQ28t"
   },
   "source": [
    "Step 3: Building the Neural Network Model\n",
    "\n",
    "We'll build a sequential neural network model. The architecture includes a\n",
    "Flatten layer to convert the 2D image data into a 1D vector, two Dense hidden layers with a ReLU activation function, and a final Dense output layer with a softmax activation for multi-class classification. The model summary provides a breakdown of the layers, their output shapes, and the number of parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 312
    },
    "id": "TELGOJSmQrXT",
    "outputId": "7a10b619-d25a-43ca-ba39-be3dcc703d9f"
   },
   "outputs": [],
   "source": [
    "# Create a sequential neural network model\n",
    "model = Sequential([\n",
    "    # Flatten the 28x28 pixel images into a 1D vector of 784 features\n",
    "    Flatten(input_shape=(28, 28)),\n",
    "    # A hidden layer with 128 neurons and ReLU activation function\n",
    "    Dense(128, activation='relu'),\n",
    "    # A second hidden layer with 64 neurons and ReLU activation\n",
    "    Dense(64, activation='relu'),\n",
    "    # The output layer with 10 neurons (one for each digit) and softmax activation for probabilities\n",
    "    Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "# Print the model summary to see the architecture and parameter count\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p-WeCJ6ZRIYE"
   },
   "source": [
    "Step 4: Compiling the Model\n",
    "\n",
    "Before training, the model must be compiled. We specify the\n",
    "optimizer, the loss function, and the metrics to monitor. The\n",
    "Adam optimizer is chosen for its adaptive learning rate, categorical cross-entropy is used as the loss function, which is suitable for multi-class classification problems, and accuracy is used as the metric to track performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YxMyvD-0Q8eb"
   },
   "outputs": [],
   "source": [
    "# Compile the model by specifying the optimizer, loss function, and evaluation metrics\n",
    "model.compile(optimizer='Adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EppbSEfrRPRc"
   },
   "source": [
    "Step 5: Training the Model\n",
    "\n",
    "The model is now trained using the prepared training data. Training involves passing the data through the network for a specified number of\n",
    "epochs (full passes through the training data), in small batches of a defined batch size. A portion of the training data is also set aside as a\n",
    "validation split to monitor the model's performance on unseen data during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wvRozL3ZRASR",
    "outputId": "790398e4-4daa-47d4-a8fc-43c11840a27d"
   },
   "outputs": [],
   "source": [
    "# Train the model on the training data\n",
    "history = model.fit(x_train, y_train,\n",
    "                    epochs=10,\n",
    "                    batch_size=32,\n",
    "                    validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gwdcqCkBRnUE"
   },
   "source": [
    "Step 6: Evaluating the Model\n",
    "\n",
    "After training, the model's performance is evaluated on the test dataset to see how well it generalizes to new, unseen data. The model.evaluate function returns the loss and accuracy on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MbyulBV7RFra",
    "outputId": "afce6b25-6caa-4cf6-ffa8-0ab4a19d21df"
   },
   "outputs": [],
   "source": [
    "# Evaluate the model on the test dataset\n",
    "test_loss, test_accuracy = model.evaluate(x_test, y_test)\n",
    "# Print the test accuracy with 4 decimal places\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zNG_5GeSR0rf"
   },
   "source": [
    "Step 7: Visualizing Training and Validation Performance\n",
    "\n",
    "To understand the training process and identify potential issues like overfitting or underfitting, we visualize the training and validation accuracy and loss over the epochs. This is done by plotting the values stored in the\n",
    "history object returned from the model.fit method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 507
    },
    "id": "CPaf6DHpRzrm",
    "outputId": "0d215639-d765-4e12-980d-618ef78532b7"
   },
   "outputs": [],
   "source": [
    "# Create a figure with two subplots side-by-side\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Plot training & validation accuracy values\n",
    "plt.subplot(1, 2, 1) # Create the first subplot for accuracy\n",
    "plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Val Accuracy')\n",
    "plt.title('Model Accuracy') # Set the title of the plot\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend() # Display the legend\n",
    "plt.grid() # Add a grid to the plot\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.subplot(1, 2, 2) # Create the second subplot for loss\n",
    "plt.plot(history.history['loss'], label='Train Loss')\n",
    "plt.plot(history.history['val_loss'], label='Val Loss')\n",
    "plt.title('Model Loss') # Set the title of the plot\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend() # Display the legend\n",
    "plt.grid() # Add a grid to the plot\n",
    "\n",
    "# Adjust subplots to give a tight layout and display the plots\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wUxswNAVSHX3"
   },
   "source": [
    "Step 8: Making Predictions and Visualizing Results\n",
    "\n",
    "Finally, we use the trained model to make predictions on the test set. The predicted labels are then compared with the true labels by visualizing a few sample images from the test set. We display each image along with its true and predicted labels to see how the model performed on individual examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "3ak2B-tQR_fI",
    "outputId": "6db23d5c-d182-4014-c163-7c9c99709a61"
   },
   "outputs": [],
   "source": [
    "# Make predictions on the test set using the trained model\n",
    "predictions = model.predict(x_test)\n",
    "\n",
    "# Display the first test image and its predicted label\n",
    "plt.figure(figsize=(5, 5))\n",
    "# Show the first test image in grayscale\n",
    "plt.imshow(x_test[0], cmap='gray')\n",
    "# Display the true and predicted labels\n",
    "plt.title(f\"True Label: {np.argmax(y_test[0])}, Predicted: {np.argmax(predictions[0])}\")\n",
    "plt.axis('off') # Hide the axis\n",
    "plt.show()\n",
    "\n",
    "# Display a grid of images with their true and predicted labels\n",
    "num_images = 9 # Number of images to display\n",
    "plt.figure(figsize=(10, 10))\n",
    "for i in range(num_images):\n",
    "    # Create a 3x3 grid of subplots\n",
    "    plt.subplot(3, 3, i + 1)\n",
    "    # Show each test image in grayscale\n",
    "    plt.imshow(x_test[i], cmap='gray')\n",
    "    # Display the true and predicted labels for each image\n",
    "    plt.title(f\"True: {np.argmax(y_test[i])}, Predicted: {np.argmax(predictions[i])}\")\n",
    "    plt.axis('off') # Hide the axis\n",
    "plt.tight_layout() # Adjust subplots to fit into figure area\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DlTeEFlATfjK"
   },
   "source": [
    "Task 1: Experiment with Different Architectures\n",
    "\n",
    "To experiment with different neural network architectures, we can modify the number of layers and neurons or change the activation functions. The code below provides implementations for three different architectures and compares their performance against the original model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "Txc3OJNOS8qH",
    "outputId": "6a95d85c-7c78-4df1-9966-9e75a095dce8"
   },
   "outputs": [],
   "source": [
    "#Task 1: Experiment with Different Architectures\n",
    "\n",
    "# Load and preprocess the data\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train = x_train.astype('float32') / 255.0\n",
    "x_test = x_test.astype('float32') / 255.0\n",
    "y_train = to_categorical(y_train, 10)\n",
    "y_test = to_categorical(y_test, 10)\n",
    "\n",
    "def create_and_train_model(layers, activations, title):\n",
    "    \"\"\"\n",
    "    Creates, compiles, and trains a new model with the specified architecture.\n",
    "\n",
    "    Args:\n",
    "        layers (list): A list of neuron counts for each hidden layer.\n",
    "        activations (list): A list of activation functions for each hidden layer.\n",
    "        title (str): The title for the performance plot.\n",
    "    \"\"\"\n",
    "    model = Sequential([Flatten(input_shape=(28, 28))])\n",
    "    for num_neurons, activation_func in zip(layers, activations):\n",
    "        model.add(Dense(num_neurons, activation=activation_func))\n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "    model.compile(optimizer='Adam',\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    print(f\"\\n--- Training Model: {title} ---\")\n",
    "    history = model.fit(x_train, y_train, epochs=10, batch_size=32, validation_split=0.2, verbose=0)\n",
    "\n",
    "    test_loss, test_accuracy = model.evaluate(x_test, y_test, verbose=0)\n",
    "    print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "    # Plotting the performance\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "    plt.plot(history.history['val_accuracy'], label='Val Accuracy')\n",
    "    plt.title(f'Model Accuracy: {title}')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['loss'], label='Train Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Val Loss')\n",
    "    plt.title(f'Model Loss: {title}')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Original Model (from the tutorial)\n",
    "create_and_train_model(layers=[128, 64], activations=['relu', 'relu'], title=\"Original Model (128, 64 neurons)\")\n",
    "\n",
    "# Experiment 1: More neurons in each layer\n",
    "create_and_train_model(layers=[256, 128], activations=['relu', 'relu'], title=\"More Neurons (256, 128 neurons)\")\n",
    "\n",
    "# Experiment 2: Deeper network (more layers)\n",
    "create_and_train_model(layers=[128, 64, 32], activations=['relu', 'relu', 'relu'], title=\"Deeper Network (128, 64, 32 neurons)\")\n",
    "\n",
    "# Experiment 3: Different activation functions\n",
    "create_and_train_model(layers=[128, 64], activations=['tanh', 'tanh'], title=\"Tanh Activation Function\")\n",
    "\n",
    "# Experiment 4: Deeper network with different activation functions\n",
    "create_and_train_model(layers=[128, 64, 32], activations=['sigmoid', 'sigmoid', 'sigmoid'], title=\"Sigmoid Activation Function\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EaaLHMMnV0Iw"
   },
   "source": [
    "Based on the outputs and plots from the previous cell:\n",
    "\n",
    "Original Model (128, 64 neurons): Achieved a test accuracy of 0.9761. The plots show good learning with some gap between training and validation accuracy/loss towards the end, suggesting slight overfitting.\n",
    "\n",
    "  More Neurons (256, 128 neurons): Achieved a test accuracy of 0.9784. This model performed ***slightly better than the original***, likely due to the increased capacity. The plots show similar trends to the original model, perhaps with a bit more overfitting.\n",
    "\n",
    "  Deeper Network (128, 64, 32 neurons): Achieved the highest test accuracy at 0.9796. ***Adding another layer with fewer neurons seems to have improved performance slightly, suggesting the extra depth helped capture more complex patterns.*** The plots show a similar pattern of slight overfitting.\n",
    "\n",
    "  Tanh Activation Function: Achieved a test accuracy of 0.9732. *Using Tanh instead of ReLU resulted in slightly lower accuracy compared to the ReLU models, indicating that ReLU might be a better choice for this specific task and architecture.* The plots show a slower initial learning phase.\n",
    "\n",
    "  Sigmoid Activation Function: Achieved a test accuracy of 0.9743. Similar to Tanh, Sigmoid also resulted in slightly lower accuracy than the ReLU models. Sigmoid can suffer from the vanishing gradient problem, which might explain the slightly lower performance. The plots show a smoother but potentially slower learning curve compared to ReLU.\n",
    "\n",
    "**In summary, for this specific MNIST classification task, increasing the number of neurons or adding another layer with ReLU activation slightly improved performance. Using Tanh or Sigmoid activation functions resulted in slightly lower accuracy compared to ReLU. The deeper network with ReLU achieved the best performance among the experimented architectures**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J_hYcwUMXI_V"
   },
   "source": [
    "Task 2: Change the Optimizer\n",
    "\n",
    "The optimizer is a crucial part of the training process, as it dictates how the model's weights are updated. Here we will use Stochastic Gradient Descent (SGD) and RMSprop as alternatives to the Adam optimizer to see how they impact the model's convergence and final accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "6VE8ABLBWn6r",
    "outputId": "ece0605d-0dc8-42e7-cf4f-6f16536cd36a"
   },
   "outputs": [],
   "source": [
    "\n",
    "from tensorflow.keras.optimizers import SGD, RMSprop\n",
    "\n",
    "# Load and preprocess the data\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train = x_train.astype('float32') / 255.0\n",
    "x_test = x_test.astype('float32') / 255.0\n",
    "y_train = to_categorical(y_train, 10)\n",
    "y_test = to_categorical(y_test, 10)\n",
    "\n",
    "def train_with_optimizer(optimizer, optimizer_name):\n",
    "    \"\"\"\n",
    "    Creates, compiles, and trains a model with a specified optimizer.\n",
    "\n",
    "    Args:\n",
    "        optimizer: The Keras optimizer object to use.\n",
    "        optimizer_name (str): The name of the optimizer for the plot titles.\n",
    "    \"\"\"\n",
    "    model = Sequential([\n",
    "        Flatten(input_shape=(28, 28)),\n",
    "        Dense(128, activation='relu'),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dense(10, activation='softmax')\n",
    "    ])\n",
    "\n",
    "    # Compile the model with the specified optimizer\n",
    "    model.compile(optimizer=optimizer,\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    print(f\"\\n--- Training with {optimizer_name} Optimizer ---\")\n",
    "    history = model.fit(x_train, y_train, epochs=10, batch_size=32, validation_split=0.2, verbose=0)\n",
    "\n",
    "    test_loss, test_accuracy = model.evaluate(x_test, y_test, verbose=0)\n",
    "    print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "    # Plotting the performance\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "    plt.plot(history.history['val_accuracy'], label='Val Accuracy')\n",
    "    plt.title(f'Model Accuracy: {optimizer_name}')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['loss'], label='Train Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Val Loss')\n",
    "    plt.title(f'Model Loss: {optimizer_name}')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Experiment 1: Adam Optimizer (Baseline)\n",
    "train_with_optimizer(Adam(), \"Adam\")\n",
    "\n",
    "# Experiment 2: SGD Optimizer\n",
    "train_with_optimizer(SGD(), \"SGD\")\n",
    "\n",
    "# Experiment 3: RMSprop Optimizer\n",
    "train_with_optimizer(RMSprop(), \"RMSprop\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2rpXrlZHZBlv"
   },
   "source": [
    "Task 3: Solve Overfitting\n",
    "\n",
    "Overfitting occurs when a model learns the training data too well, including its noise and outliers, which negatively impacts its performance on new, unseen data. We can mitigate this using Dropout regularization, a technique that randomly \"drops out\" neurons during training, forcing the network to learn more robust features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 559
    },
    "id": "o3VSajvFX7q9",
    "outputId": "9da332a5-092b-4d89-ef2b-a00f03b80c0c"
   },
   "outputs": [],
   "source": [
    "\n",
    "from tensorflow.keras.layers import Dropout\n",
    "\n",
    "# Load and preprocess the data\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train = x_train.astype('float32') / 255.0\n",
    "x_test = x_test.astype('float32') / 255.0\n",
    "y_train = to_categorical(y_train, 10)\n",
    "y_test = to_categorical(y_test, 10)\n",
    "\n",
    "# Create a model with Dropout layers to combat overfitting\n",
    "model_with_dropout = Sequential([\n",
    "    # Flatten the 28x28 images to a 1D vector\n",
    "    Flatten(input_shape=(28, 28)),\n",
    "    # First hidden layer with 128 neurons and ReLU activation\n",
    "    Dense(128, activation='relu'),\n",
    "    # Dropout layer, dropping 50% of the neurons during training\n",
    "    Dropout(0.5),\n",
    "    # Second hidden layer with 64 neurons and ReLU activation\n",
    "    Dense(64, activation='relu'),\n",
    "    # Dropout layer, dropping 30% of the neurons\n",
    "    Dropout(0.3),\n",
    "    # Output layer with 10 neurons and softmax activation\n",
    "    Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the new model\n",
    "model_with_dropout.compile(optimizer='Adam',\n",
    "                           loss='categorical_crossentropy',\n",
    "                           metrics=['accuracy'])\n",
    "\n",
    "# Train the model with Dropout\n",
    "print(\"\\n--- Training Model with Dropout for Overfitting Mitigation ---\")\n",
    "history_dropout = model_with_dropout.fit(x_train, y_train,\n",
    "                                         epochs=20, # Increase epochs to see the effect of dropout\n",
    "                                         batch_size=32,\n",
    "                                         validation_split=0.2,\n",
    "                                         verbose=0)\n",
    "\n",
    "# Evaluate the model\n",
    "test_loss_dropout, test_accuracy_dropout = model_with_dropout.evaluate(x_test, y_test, verbose=0)\n",
    "print(f\"Test Accuracy with Dropout: {test_accuracy_dropout:.4f}\")\n",
    "\n",
    "# Plotting the performance to visualize the effect of dropout\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history_dropout.history['accuracy'], label='Train Accuracy (with Dropout)')\n",
    "plt.plot(history_dropout.history['val_accuracy'], label='Val Accuracy (with Dropout)')\n",
    "plt.title('Model Accuracy with Dropout')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history_dropout.history['loss'], label='Train Loss (with Dropout)')\n",
    "plt.plot(history_dropout.history['val_loss'], label='Val Loss (with Dropout)')\n",
    "plt.title('Model Loss with Dropout')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NfBjZD6OanHh"
   },
   "source": [
    "Task 3: with Dropout and EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "wX0o39p3aGpm",
    "outputId": "76a45102-6fa6-498f-9c7d-442a55c2b0f7"
   },
   "outputs": [],
   "source": [
    "\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Load and preprocess the data\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train = x_train.astype('float32') / 255.0\n",
    "x_test = x_test.astype('float32') / 255.0\n",
    "y_train = to_categorical(y_train, 10)\n",
    "y_test = to_categorical(y_test, 10)\n",
    "\n",
    "# Create a model with Dropout layers to combat overfitting\n",
    "model_with_dropout = Sequential([\n",
    "    # Flatten the 28x28 images to a 1D vector\n",
    "    Flatten(input_shape=(28, 28)),\n",
    "    # First hidden layer with 128 neurons and ReLU activation\n",
    "    Dense(128, activation='relu'),\n",
    "    # Dropout layer, dropping 50% of the neurons during training\n",
    "    Dropout(0.5),\n",
    "    # Second hidden layer with 64 neurons and ReLU activation\n",
    "    Dense(64, activation='relu'),\n",
    "    # Dropout layer, dropping 30% of the neurons\n",
    "    Dropout(0.3),\n",
    "    # Output layer with 10 neurons and softmax activation\n",
    "    Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the new model\n",
    "model_with_dropout.compile(optimizer='Adam',\n",
    "                           loss='categorical_crossentropy',\n",
    "                           metrics=['accuracy'])\n",
    "\n",
    "# Implement Early Stopping to prevent overfitting\n",
    "# Monitor validation loss, and stop if it doesn't improve for 5 epochs\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, verbose=1)\n",
    "\n",
    "# Train the model with both Dropout and Early Stopping\n",
    "print(\"\\n--- Training Model with Dropout and Early Stopping ---\")\n",
    "history_dropout = model_with_dropout.fit(x_train, y_train,\n",
    "                                         epochs=50, # Set a high number of epochs; Early Stopping will halt training\n",
    "                                         batch_size=32,\n",
    "                                         validation_split=0.2,\n",
    "                                         callbacks=[early_stopping], # Pass the early stopping callback\n",
    "                                         verbose=1)\n",
    "\n",
    "# Evaluate the final model\n",
    "test_loss_dropout, test_accuracy_dropout = model_with_dropout.evaluate(x_test, y_test, verbose=0)\n",
    "print(f\"\\nTest Accuracy with Dropout and Early Stopping: {test_accuracy_dropout:.4f}\")\n",
    "\n",
    "# Plotting the performance to visualize the effect of regularization\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history_dropout.history['accuracy'], label='Train Accuracy (with Dropout)')\n",
    "plt.plot(history_dropout.history['val_accuracy'], label='Val Accuracy (with Dropout)')\n",
    "plt.title('Model Accuracy with Dropout')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history_dropout.history['loss'], label='Train Loss (with Dropout)')\n",
    "plt.plot(history_dropout.history['val_loss'], label='Val Loss (with Dropout)')\n",
    "plt.title('Model Loss with Dropout')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y1w5r1vvb3rs"
   },
   "source": [
    "Looking at the plots from the last two coding blocks:\n",
    "\n",
    "Model with Dropout, 20 epochs:\n",
    "\n",
    " Overfitting/Underfitting: You can observe a gap between the training accuracy and validation accuracy, and between the training loss and validation loss, especially in the later epochs. The training accuracy continues to increase while the validation accuracy plateaus or even slightly decreases. Similarly, the training loss continues to decrease while the validation loss plateaus or increases. This indicates that the model is overfitting to the training data.\n",
    " Effect of Epochs: With 20 epochs, the model has had enough time to learn the training data extensively, leading to overfitting. The validation performance starts to degrade after a certain number of epochs where the model is no longer generalizing well to unseen data.\n",
    "\n",
    "Model with Dropout and Early Stopping, up to 50 epochs:\n",
    "\n",
    "  Overfitting/Underfitting: Compared to the previous plot, the gap between the training and validation curves is smaller. The validation loss also stops decreasing and starts to increase, which is the point where early stopping intervenes. This indicates that the model is still showing some signs of overfitting, but Early Stopping has helped to mitigate it by stopping the training before severe overfitting occurs.\n",
    "  Effect of Epochs: By setting a high number of epochs (50) and using Early Stopping, we allow the model to train until the validation loss stops improving for a specified number of epochs (patience=5). This prevents the model from training for too long and overfitting excessively. The training stops at Epoch 17, indicating that further training would likely lead to worse performance on unseen data.\n",
    "\n",
    "In summary, the plots clearly show that without regularization and early stopping, the model tends to overfit. The introduction of Dropout helps to reduce overfitting, and Early Stopping further prevents it by stopping the training process at the optimal point based on the validation performance."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
