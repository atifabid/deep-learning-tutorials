{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 2 Multi-layered perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T4-iUUViIDZR"
   },
   "source": [
    "**Step 1: Importing necessary libraries and packages**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "93l_4Z52Fc9j"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w47xCL5QIbAx"
   },
   "source": [
    "**Step 2: Loading and Splitting the Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_Fzp4QtNG9D8"
   },
   "outputs": [],
   "source": [
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data # Features\n",
    "y = iris.target # Labels\n",
    "\n",
    "# Split the data into training and testing sets (70% training, 30% testing)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TEnn7imcIl0G"
   },
   "source": [
    "**Step 3: Data Scaling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U8qaJyChHBY0"
   },
   "outputs": [],
   "source": [
    "# Standardize the features to have a mean of 0 and a standard deviation of 1\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train) # Fit on training data and transform\n",
    "X_test_scaled = scaler.transform(X_test) # Transform test data (using same scaling)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0gEAR22hIuyT"
   },
   "source": [
    "**Step 4: Creating and Training the MLP Classifier**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 80
    },
    "id": "2LX9Nht_HFPq",
    "outputId": "1c8c7a9e-75c4-4a36-fd9c-f3a4c59b66ae"
   },
   "outputs": [],
   "source": [
    "# Create an MLP classifier with two hidden layers of 10 neurons each\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(10, 10), max_iter=1000, random_state=42, learning_rate_init=0.001)\n",
    "\n",
    "# Train the MLP classifier on the scaled training data\n",
    "mlp.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T7Gqi2vJI7oG"
   },
   "source": [
    "**Step 5: Making Predictions and Evaluating the Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BoHjc0WlHKrG",
    "outputId": "8d26ca56-a133-47e5-b1c6-248668a7cb73"
   },
   "outputs": [],
   "source": [
    "# Predict the test set results\n",
    "y_pred = mlp.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy:.2f}')\n",
    "\n",
    "# Print detailed classification report\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n8iLBpAWJGge"
   },
   "source": [
    "**Step 6: Displaying the MLP Structure and Training Information**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rS9mIV58HRYV",
    "outputId": "423521c8-16c3-4609-d111-14a3f396bd63"
   },
   "outputs": [],
   "source": [
    "# Display the structure of the MLP classifier\n",
    "print(\"\\nMLP Structure:\")\n",
    "print(f\"Number of layers: {mlp.n_layers_}\")\n",
    "print(f\"Number of outputs: {mlp.n_outputs_}\")\n",
    "print(f\"Activation function: {mlp.activation}\")\n",
    "print(f\"Output activation function: {mlp.out_activation_}\")\n",
    "print(f\"Number of epochs: {mlp.n_iter_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hx5WIygUJSMh"
   },
   "source": [
    "**Step 7: Visualizing the Learning Curve**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 564
    },
    "id": "D8F6Q07RHWTF",
    "outputId": "149915b6-df5d-4984-d391-c55305cd1cf3"
   },
   "outputs": [],
   "source": [
    "# Plot the loss curve\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(mlp.loss_curve_, label='Training Loss')\n",
    "plt.title('MLP Classifier Learning Curve')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FF6Y-nyBJY5t"
   },
   "source": [
    "**Task 1: Experiment with Different Architectures**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "xST9O33dHbnd",
    "outputId": "acf0c9ec-91fc-4c21-b1fd-77d6e7502c61"
   },
   "outputs": [],
   "source": [
    "# Task 1: Experiment with Different Architectures\n",
    "\n",
    "# Define a function to train and evaluate a model with a given architecture\n",
    "def train_and_evaluate(hidden_layer_sizes, activation='relu'):\n",
    "    print(f\"\\n--- Training with hidden layers: {hidden_layer_sizes}, activation: {activation} ---\")\n",
    "    mlp = MLPClassifier(hidden_layer_sizes=hidden_layer_sizes, activation=activation, max_iter=1000, random_state=42, learning_rate_init=0.001)\n",
    "    mlp.fit(X_train_scaled, y_train)\n",
    "    y_pred = mlp.predict(X_test_scaled)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f\"Accuracy: {accuracy:.2f}\")\n",
    "    print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(mlp.loss_curve_, label='Training Loss')\n",
    "    plt.title(f'Learning Curve with Hidden Layers {hidden_layer_sizes} and {activation} Activation')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "    return accuracy\n",
    "\n",
    "# Experiment with different numbers of neurons and layers\n",
    "architectures = [(5, ), (20, 10), (50, 20, 10)]\n",
    "for arch in architectures:\n",
    "    train_and_evaluate(arch)\n",
    "\n",
    "# Experiment with different activation functions\n",
    "train_and_evaluate(hidden_layer_sizes=(10, 10), activation='tanh')\n",
    "train_and_evaluate(hidden_layer_sizes=(10, 10), activation='logistic')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U5At2uq9LuHp"
   },
   "source": [
    "How does increasing the number of layers or neurons affect the\n",
    "accuracy and learning curve?\n",
    "What configuration gives the best performance?\n",
    "\n",
    "Based on the output from the experiment with different architectures:\n",
    "\n",
    "Number of layers and neurons: Increasing the number of layers or neurons (from 5 neurons in one layer to 20, 10 neurons in two layers, and then to 50, 20, 10 neurons in three layers) did not significantly change the accuracy in this case; all three architectures achieved 1.00 accuracy on the test set. The learning curves show that all models converged, although the simplest model with one hidden layer of 5 neurons took slightly longer to converge and had a less smooth loss curve.\n",
    "\n",
    "Activation functions: Experimenting with different activation functions showed that tanh also achieved 1.00 accuracy, similar to relu. However, the logistic activation function resulted in a slightly lower accuracy of 0.98 and showed a warning about not converging within the maximum iterations, indicating it might not be as suitable for this dataset with the given parameters.\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "In this particular experiment, all tested architectures with relu and tanh activation achieved perfect accuracy on the test set. The simplest architecture with one hidden layer of 5 neurons, while showing a slightly less smooth learning curve, also achieved 1.00 accuracy, suggesting that for this dataset, a very complex model might not be necessary.\n",
    "\n",
    "Given the performance on the test set, the configuration with hidden_layer_sizes=(5,) and activation='relu' or activation='tanh' would be considered among the best performing and also the simplest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vqn9DbIjJv2I"
   },
   "source": [
    "**Task 2: Change the Learning Rate**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "61W2_ddtHr1N",
    "outputId": "8c329945-87d0-4a8b-9cda-c9c1b671582d"
   },
   "outputs": [],
   "source": [
    "# Task 2: Change the Learning Rate\n",
    "\n",
    "# Define a function to train and evaluate a model with a given learning rate\n",
    "def train_with_learning_rate(learning_rate):\n",
    "    print(f\"\\n--- Training with learning rate: {learning_rate} ---\")\n",
    "    mlp = MLPClassifier(hidden_layer_sizes=(10, 10), max_iter=1000, random_state=42, learning_rate_init=learning_rate)\n",
    "    mlp.fit(X_train_scaled, y_train)\n",
    "    y_pred = mlp.predict(X_test_scaled)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f\"Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(mlp.loss_curve_, label='Training Loss')\n",
    "    plt.title(f'Learning Curve with Learning Rate {learning_rate}')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "# Experiment with different learning rates\n",
    "learning_rates = [0.1, 0.01, 0.0001]\n",
    "for lr in learning_rates:\n",
    "    train_with_learning_rate(lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WxioafD8NlMO"
   },
   "source": [
    "How does changing the learning rate affect the loss curve and the number of epochs? What learning rate provides the best balance between convergence speed and model performance?\n",
    "\n",
    "Based on the output from the experiment with different learning rates:\n",
    "\n",
    "Effect on loss curve and epochs:\n",
    "        A learning rate of 0.1 shows a rapid initial decrease in loss, but the curve is less smooth and might overshoot the minimum. It also converged quickly with fewer epochs.\n",
    "        A learning rate of 0.01 shows a smoother and more gradual decrease in loss, converging effectively and achieving high accuracy.\n",
    "        A learning rate of 0.0001 shows a very slow decrease in loss and a warning about not converging within the maximum iterations, indicating that it would require many more epochs to potentially reach a better loss.\n",
    "        \n",
    "Best balance:\n",
    "        A learning rate of 0.01 appears to provide the best balance between convergence speed and model performance in this experiment. It achieved the highest accuracy (1.00) among the tested learning rates and the loss curve indicates a stable and effective convergence without requiring an excessive number of epochs. While a learning rate of 0.1 also achieved good accuracy (0.96), the less smooth loss curve suggests potential instability during training. The very low learning rate of 0.0001 resulted in poor performance and slow convergence.\n",
    "\n",
    "    \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
